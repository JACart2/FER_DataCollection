import cv2

from fer import FER

## faces

paths = [r'C:\Users\gr8jj\OneDrive\Desktop\SPRING 2025\CS497\fake_faces\neutral_face.jpg',
r'C:\Users\gr8jj\OneDrive\Desktop\SPRING 2025\CS497\fake_faces\angry_face.jpg',
r'C:\Users\gr8jj\OneDrive\Desktop\SPRING 2025\CS497\fake_faces\disgusted_face.jpg',
r'C:\Users\gr8jj\OneDrive\Desktop\SPRING 2025\CS497\fake_faces\happy_face.jpg',
r'C:\Users\gr8jj\OneDrive\Desktop\SPRING 2025\CS497\fake_faces\surprised_face.jpg']


def detect_faces(image):
    """Utilizes the Haar cascade classifier (opencv) to detect faces.
        It calculates and scales the image and grayscales it for better accuracy.

    Args:
        image (numpy.ndarray): Image object (one frame) generated by ZED camera

    Returns:
        numpy.ndarray: coordinates of the detected face
            >>> [[681 265 492 492]]

    """
    # load pre-trained model (haarcascade)
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Detect faces in the grayscale image using the detectMultiScale method of the face cascade
    # scaleFactor: Parameter specifying how much the image size is reduced at each image scale.
    # minNeighbors: Parameter specifying how many neighbors each rectangle should have to retain it.
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)

    # Return the coordinates of the detected faces (x, y, width, height)
    return faces


def detect_emotion(image, faces):
    """Detects emotion given image n-array, and face coords.

    Args:
        image (numpy.ndarray): the image

        faces (numpy.ndarray): coordinates of the face

    Returns:
        Type: definition
            >>> Example

    """
    emotions = {}

    # Initialize the FER (Facial Expression Recognition) detector with MTCNN (Multi-Task Cascaded Convolutional Networks) 
    detector = FER(mtcnn=True)

    # Convert the ZED image to a format usable by OpenCV
    image_cv2 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Iterate over each detected face
    for i, (x, y, w, h) in enumerate(faces):
        # Crop the face region from the image
        face_img = image_cv2[y:y+h, x:x+w]

        # Detect emotion for the face using the FER detector
        emotion, _ = detector.top_emotion(face_img)

        # Store the detected emotion for the current face in the emotions dictionary
        emotions[f"Passenger {i+1}"] = emotion

        # Draw bounding box around the detected face
        cv2.rectangle(image_cv2, (x, y), (x+w, y+h), (255, 0, 0), 2)

        # Put text indicating the emotion on the image
        cv2.putText(image_cv2, f"Passenger {i+1}: {emotion}", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)

    # Return the modified image with bounding boxes and emotion labels, along with the emotions dictionary
    return image_cv2, emotions



for path in paths:
    image = cv2.imread(path)

    # Check if the image was loaded successfully
    if image is None:
        print("Error: Could not load image.")
    else:
        
        face_coords = detect_faces(image)

        response = detect_emotion(image, face_coords)

        cv2.imshow('Image', response[0])
        print(response[1])

        # Wait for any key to close the window
        cv2.waitKey(0)
        cv2.destroyAllWindows()
